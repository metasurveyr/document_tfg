---
bibliography: [references.bib]
---

```{css}
#| echo: false
p {
  text-align: justify
}
```

```{r}
#| echo: false
#| label: setup

source("../_common.R")
```

```{r}
#| results: "asis"
#| echo: false
#| label: status-chapter
```

# Marco teórico {#sec-Chapter2}

Este capítulo desarrolla el fundamento teórico esencial para la comprensión de la inferencia estadística en el contexto de muestreo de poblaciones finitas, un pilar fundamental en la generación de indicadores sociales y estadística oficial. Se presenta una progresión sistemática y rigurosa desde los conceptos fundamentales hasta las técnicas más sofisticadas de estimación de varianza, con especial énfasis en su aplicación práctica en encuestas socioeconómicas complejas.

El marco teórico se estructura en tres componentes principales interrelacionados:

1. La teoría fundamental de inferencia en muestreo, incluyendo los principios probabilísticos subyacentes
2. Los métodos de estimación de parámetros poblacionales y sus propiedades estadísticas asintóticas
3. Las técnicas específicas para el tratamiento de diseños muestrales complejos y sus implicaciones metodológicas

Esta base teórica rigurosa resulta fundamental no solo para comprender la implementación de los métodos de estimación de varianza en el paquete `metasurvey`, sino también para garantizar la validez estadística y robustez de las inferencias realizadas.

## Inferencia en muestreo de poblaciones finitas 

Las encuestas por muestreo constituyen el pilar metodológico para la construcción de indicadores socio-demográficos y económicos en la estadística oficial contemporánea. La inferencia en este contexto demanda no solo estimaciones puntuales precisas, sino también una cuantificación rigurosa de su variabilidad e incertidumbre asociada, elementos esenciales para establecer la confiabilidad y validez de las estimaciones y permitir una inferencia estadística robusta fundamentada en principios matemáticos sólidos.

### Diseño muestral

El diseño muestral representa la piedra angular del proceso inferencial en poblaciones finitas, constituyendo el mecanismo probabilístico que determina la selección de unidades muestrales y, consecuentemente, las propiedades estadísticas fundamentales de los estimadores derivados.

La definición matemática se basa en que dado un universo $U$ de $N$ elementos (puede ser conocido o no) $\{u_{1},u_{2}, \cdots, u_{N}\}$ y se considera un conjunto de tamaño $n$ de elementos de $U$ que se denota como $s = \{u_{1},u_{2}, \cdots, u_{n}\}$ al cual comúnmente denominamos **muestra**, el diseño muestral puede definirse de la siguiente forma:

$$
Pr(S = s) = p(s)
$$

Realizando un poco de inspección en la definición anterior se puede observar que el diseño muestral es una función de probabilidad que asigna una probabilidad a cada subconjunto de $U$ de tamaño $n$. En este sentido, es posible definir diferentes tipos de diseño, entre ellos los más comunes:.

-   **Diseño Aleatorio Simple (SI)**

El diseño aleatorio simple es el diseño más sencillo y se define de la siguiente forma:

$$
p(s) = \frac{1}{\binom{N}{n}}
$$

*Ejemplo*: Si se tiene una población de 1000 individuos y se desea seleccionar una muestra de 100 de manera aleatoria, cada combinación de 100 individuos tiene la misma probabilidad de ser seleccionada.

-   **Diseño Bernoulli (BE)**

El (**BE**) es un diseño sencillo que se utiliza cuando se desea seleccionar una muestra de un universo de tamaño $N$ además de considerar una probabilidad de inclusión $\pi$ para cada elemento de $U$. Se define el diseño Bernoulli de la siguiente forma:

$$
p(s) = \underbrace{\pi \times \pi \times \cdots \times \pi}_{n_{s}} \times \underbrace{(1-\pi) \times (1-\pi) \times \cdots \times (1-\pi)}_{N-n_{s}} = \pi ^{n_{s}} (1-\pi)^{N-n_{s}}
$$

Una diferencia fundamental entre el diseño (*BE*) y el diseño **SI** es que en el **BE** el tamaño de muestra es aleatorio y su distribución es binomial, mientras que en el diseño SI el tamaño de muestra es fijo.

-   **Diseño Estratificado (ST)**

El diseño estratificado es un diseño que se utiliza cuando se desea seleccionar una muestra de tamaño $n$ de un universo de tamaño $N$ donde además se quiere dividir el universo en $H$ estratos $U_{1}, U_{2}, \cdots, U_{H}$. Dentro de cada estrato se selecciona una muestra de tamaño $n_{h}$ y se define el diseño estratificado de la siguiente forma:

$$
p(s) = \prod_{l=1}^{H} p(s_{H})
$$

En cada estrato se puede utilizar un diseño diferente pero en general se utiliza el diseño **SI**, más conocido **STSI** (Stratified Simple Random Sampling). En este caso cada $p_{h}(s_{h})$ es el diseño aleatorio simple en el estrato $h$.

### Probabilidades de Inclusión y el Estimador de Horvitz-Thompson {#sec-ht}

Una vez definido el concepto de diseño muestral es posible definir la probabilidad de que un elemento de la población sea seleccionado en la muestra, esta probabilidad se conoce como probabilidad de inclusión y se define de la siguiente forma:

-   **Probabilidad de inclusión de primer orden**

$$
\pi_{k} = Pr(u_{k} \in s) = Pr(I_{k} = 1) 
$$

Donde $I_{k}$ es una variable aleatoria que toma el valor de 1 si el elemento $u_{k}$ es seleccionado en la muestra y 0 en caso contrario. Definir estas variables indicadoras son de utilidad para entender el comportamiento de los estimadores bajo el diseño muestral y nos permite definir los estimadores en $U$ y no en $S$. Es claro que $I_{k} \sim Bernoulli(\pi_{k})$ y $E(I_{k}) = Pr(I_{k}) = \pi_{k}$.

Esta probabilidad es importante ya que es la la base para la construcción de estimadores insesgados y eficientes, en este sentido, es posible definir el estimador de Horvitz-Thompson (**HT**) para estimar un total $t = \sum_{U} {t_{k}}$ de la siguiente forma:

$$
\hat{t}_{y} = \sum_{k=1}^{N} \frac{y_{k}}{\pi_{k}} \times I_{k}
$$

Este estimador es propuesto por Horvitz y Thompson en 1952 y es un estimador insesgado en el diseño, en el sentido de que $E(\hat{t}_{y}) = t$ y es eficiente en el sentido de que $Var(\hat{t}_{y})$ es el menor posible entre los estimadores insesgados. Este estimador es muy utilizado en la práctica y es la base para la construcción de otros estadísticos,   como medias, proporciones, varianzas, entre otros. Para más detalles sobre las propiedades de Horvitz-Thompson (**HT**) se puede consultar en [@särndal2003] y [@horvitz1952].

### Ponderación basada en el diseño y estimadores más comunes

En general es utilizado el concepto de ponderador para realizar estimaciones de totales, medias, proporciones, varianzas, entre otros. En este sentido, es posible definir el ponderador inducido por el diseño muestral de la siguiente forma:

$$
w_{k} = \frac{1}{\pi_{k}}
$$

Este ponderador puede interpretarse como el número individuos que representa el individuo $k$ en la población. Este valor es el que comúnmente se publica junto a los microdatos y el estándar en los diferentes softwares para procesar encuestas. Junto al estimador de un total es posible definir el estimador de un promedio, proporción o razón en el contexto de la $\pi$-expansión.

#### Estimador de un promedio {.unnumbered}

$$
\hat{\bar{y}} = \frac{\sum_{k=1}^{N} w_{k} I_{k} y_{k}}{\sum_{k=1}^{N} w_{k} I_{k}}
$$

Este estimador puede ser utilizados en encuestas de hogares, donde se desea estimar el ingreso promedio de los hogares de una región de forma anual, o mensual.

#### Estimador de una proporción {.unnumbered}

$$
\hat{p} = \frac{\sum_{k=1}^{N} I_{k} w_{k} y_{k}}{\sum_{k=1}^{N} w_{k} I_{k}} = \frac{\sum_{k=1}^{N} I_{k} w_{k} y_{k}}{\hat{N}}
$$

Puede ser de interés estimar la proporción de hogares que tienen acceso a internet en una región, en este caso se puede utilizar el estimador de proporción.

#### Estimador de una razón {.unnumbered}

Se quiere estimar la razón $R = \frac{\sum_{k=1}^{N} y_{k}}{\sum_{k=1}^{N} z_{k}}$. Siendo $z_{k}$ otra variable dentro del conjunto de datos. En este caso se puede definir el estimador de la razón de la siguiente forma:

$$
\hat{R} = \frac{\sum_{k=1}^{N} w_{k} y_{k}}{\sum_{k=1}^{N} w_{k}z_{k}}
$$

El estimador de razón es utilizado para construir variables de mercado de trabajo como la tasa de desempleo, tasa de ocupación, entre otros.

#### Inferencia sobre el tamaño de la población {.unnumbered}

Una vez definidos los estimadores, podemos ver que los estimadores de medias y proporciones son un caso particular del estimador de razón. Un detalle no menor es que asumimos $N$ fijo pero desconocido, por esto al realizar proporciones se ajusta el total sobre un estimador del tamaño de la población:

$$
\hat{N} = \sum_{k=1}^{N} I_{k}w_{k}
$$

Existen diseños denominados **auto-ponderados** donde por definición $\sum_{k=1}^{N} w_{k} = N$, en este caso particular el estimador de medidas y proporciones es un caso particular del estimador de total, ya que el estadístico puede definirse de la siguiente forma:

$$
\hat{\bar{y}}_{s} = \frac{\sum_{k=1}^{N} I_{k} w_{k} y_{k}}{\sum_{k=1}^{N} w_{k} I_{k}} = \frac{\sum_{k=1}^{N} I_{k} w_{k} y_{k}}{N} = \frac{1}{N} \times \sum_{k=1}^{N} I_{k} w_{k} y_{k} = a \times \hat{t}_{y}
$$

### Medidas de incertidumbre y errores estándar

Se puede medir la variabilidad de los estimadores y calcular su varianza. Para detalles completos de los cálculos de varianza, consulte el Apéndice A.

#### Momentos muéstrales y estimadores de varianza

Para un estadístico $\theta$, su varianza bajo un diseño muestral $p(s)$ se define como:

$$
V(\hat{\theta}) = E((\theta - E(\hat{\theta}))^{2}) = \sum_{s \in S}{p(s)\left(\hat{\theta}_{s} - E(\hat{\theta}_{s})\right)}
$$

La forma de calcular la varianza depende del estimador $\hat{\theta}$. Por ejemplo, para el estimador de varianza de un total, se utiliza la siguiente fórmula:

$$
V(\hat{t}_{y}) = \sum_{U}{V(I_{k} \times y_{k} \times w_{k})} + \sum_{U}{\sum_{k \not{=} l }{Cov(I_{k} \times y_{k} \times w_{k}, I_{l} \times y_{l} \times w_{l})}}
$$

Después de simplificar, obtenemos:

$$
V(\hat{t}_{y}) = \sum_{U}{V(I_{k}) \times w_{k} \times y_{k}^{2}} + \sum_{U}{\sum_{k \not{=} l }{Cov(I_{k}, I_{l}) \times y_{k} \times w_{k} \times y_{l}  \times w_{l} }}
$$

Donde definimos las siguientes identidades para simplificar cálculos:

$$
Cov(I_{k}, I_{l}) = \Delta_{kl} = \pi_{kl} - \pi_{k} \times \pi_{l}
$$

$$
\check{y}_{k} = y_{k} \times w_{k}
$$

$$
\check{\Delta}_{kl} = \Delta_{kl} \times \frac{1}{\pi_{kl}} = \Delta_{kl} \times w_{kl}
$$

Una vez definida la varianza del estimador, necesitamos estimar su varianza. Para esto, utilizamos la técnica de $\pi$-expansión. Después de algunas manipulaciones algebraicas, obtenemos la varianza del estimador:

$$
V(\hat{t}_{y}) = \sum_{U}{\check{y}_{k}^{2}} + \sum_{U}{\sum_{k \not{=} l } \Delta_{kl} \times \check{y}_{k} \times \check{y}_{l} } = \sum_{U}{\sum{\Delta_{kl} \times \check{y}_{k} \times \check{y}_{l} }}
$$

Podemos verificar que este estimador de varianza es insesgado con las definiciones de $E(I_{k}I_{l})$ y tomando esperanzas. Es decir, se verifica que $E(\hat{V}(\hat{t}_{y})) = V(\hat{t}_{y})$. Al ser un estimador insesgado, su eficiencia depende del diseño muestral y de la varianza de los ponderadores, es decir, de la varianza de las probabilidades de inclusión. En algunos casos, es donde entra en juego dividir grupos heterogéneos en estratos o realizar muestreos en varias etapas.

Para el caso de un estimador de un promedio, la varianza se define de la siguiente forma: 
$$
V(\hat{\bar{y}}) = \frac{1}{N^{2}} \times \sum_{U}{\sum_{k \not{=} l } \Delta_{kl} \times \check{y}_{k} \times \check{y}_{l} }
$$

Esto es válido en el caso de contar con un tamaño de población conocido. En otro caso, el estimador de la media no es un estimador lineal y para calcular su varianza deben optarse por métodos de estimación de varianzas alternativos como el de linealización de Taylor.

Es importante considerar que en esta sección se presenta un caso ideal donde la muestra es obtenida de un listado **perfecto** de la población objetivo denominado **marco muestral**. En la práctica, el marco muestral es imperfecto y se debe considerar la no respuesta, la cobertura y la falta de actualización del marco. En general, los microdatos publicados incluyen ciertos ponderadores que no son precisamente los ponderadores originales definidos en la sección anterior, sino que son sometidos a un proceso de **calibración** donde se intenta ajustar a ciertas variables de control y mejorar problemas causados por la no respuesta. Al realizar el proceso de calibración, los ponderadores calibrados son lo más cercano posible a los ponderadores originales, de forma que si los ponderadores originales son insesgados, los ponderadores calibrados serán próximos a ser insesgados.

En la práctica, para diseños complejos no se dispone de las probabilidades de selección de segundo orden, insumo principal para calcular los errores estándar como se expuso en las formulaciones anteriores. Por esto, se requiere optar por metodologías alternativas como el método del último conglomerado, método de replicación jackknife, método de bootstrap, entre otros. En este sentido, es importante tener en cuenta que la varianza de los estimadores es un componente fundamental para realizar inferencias y cuantificar la confiabilidad de los resultados.

En resumen, para realizar estimaciones puntuales ya sean totales, medias, proporciones o razones, simplemente debemos ponderar los datos con los estadísticos anteriormente mencionados. Pero para realizar un proceso de inferencia completo se requiere calcular sus errores estándar, construir intervalos de confianza y/o poder medir la estabilidad de nuestros resultados. En este sentido, es importante tener al alcance herramientas que permitan realizar este tipo de cálculos, ya que en diferentes softwares estadísticos junto a la estimación puntual se presentan los errores estándar asumiendo diseños sencillos ya sea por omisión del usuario o por limitaciones de los paquetes estadísticos.



En base a lo expuesto en la sección anterior, es posible definir los errores estándar de los estimadores de forma teórica. Sin embargo, en la práctica, la estimación de la varianza de los estimadores es un problema complejo, especialmente en diseños de muestreo multietápicos y complejos. En estos casos, las probabilidades de inclusión de segundo orden son difíciles de calcular y se requieren métodos alternativos para estimar la varianza de los estimadores.

Cada estimador tiene asociado un error estándar que permite cuantificar la variabilidad de la estimación, debido a que la muestra es aleatoria esta medida es una variable aleatoria. Dentro de la incertidumbre puede separarse en errores muestrales y no muestrales. Los primeros refieren a la variabilidad de la estimación debido a la selección de la muestra y los segundos refieren a la variabilidad de la estimación debido a errores de medición, errores de no respuesta, entre otros [@särndal2003].

En este trabajo se centra en la estimación de los errores muestrales, ya que los errores no muestrales son difíciles de cuantificar. Los errores muestrales se pueden cuantificar mediante la varianza de la estimación. Esta varianza depende del diseño muestral ya que, como se mencionó anteriormente, el diseño muestral induce propiedades estadísticas claves como la distribución en el muestreo, valores esperados y varianzas de estimadores poblacionales. El paquete `survey` permite estimar la varianza de la estimación de forma sencilla y eficiente, sin embargo, en algunos casos la estimación de la varianza no es correcta, ya que el paquete `survey` asume un muestreo simple con probabilidades de inclusión desiguales y con reposición, es decir, con una fracción de muestreo $f = \frac{n}{N} \approx 0$ [@lumley2004].

Para diseños multietápicos, las probabilidades de segundo orden son muy complejas de calcular, por lo que una estimación directa no es muy factible. Además, estos ponderadores no son exactamente los pesos muestrales definidos en los capítulos anteriores, ya que se ajustan para tener en cuenta la no respuesta y la calibración, lo cual permite una estimación más precisa de ciertas variables de interés. En el caso de que se cuente con un mecanismo para obtener las probabilidades de inclusión de segundo orden, este no tendría en cuenta el proceso posterior de calibración, por lo que la estimación de la varianza no sería correcta.

En general, para este tipo de casos se utilizan principalmente las siguientes estrategias: el método del último conglomerado, donde se asume que la variabilidad proviene únicamente de la selección en la primera etapa, y métodos de remuestreo como el Bootstrap o Jackknife. En este trabajo se propone la implementación de forma nativa de diferentes métodos utilizando solamente un argumento al cargar la encuesta, permitiendo a usuarios no expertos en metodología de muestreo obtener estimaciones de varianzas correctas y confiables.

Adicionalmente, para estimadores no lineales se utiliza el método de Linearización de Taylor, que permite aproximar el estimador como función de estimadores lineales. Un caso típico es la tasa de desempleo, que se calcula como el cociente entre la población desocupada y la población económicamente activa. En este caso, se puede aproximar la tasa de desempleo como función de estimadores lineales y obtener una estimación de la varianza de la tasa de desempleo o, de forma similar, un estimador de medias o proporciones.

### Métodos de remuestreo

La estimación del error estándar de una media u otros resúmenes poblacionales se basa en la desviación estándar de dicho estimador a través de múltiples muestras independientes. Sin embargo, en encuestas reales solo se cuenta con una muestra. El enfoque de **pesos replicados** ofrece una alternativa, al calcular la variabilidad del estimador a partir de múltiples subconjuntos que se comportan de manera parcialmente independiente, y luego extrapolar esta variabilidad para obtener una estimación que se asemeje a la que se obtendría si se tuvieran múltiples muestras independientes.

#### Réplicas de Mitad de Muestra {.unnumbered}

Para entender mejor este método, se puede considerar un diseño estratificado en el que se seleccionan dos unidades por estrato. Si se dividen los datos en dos mitades, tomando una unidad de cada estrato, se crean subconjuntos que se pueden considerar como "mitades" independientes. Si la corrección por población finita no es relevante, la varianza de un estimador basado en una mitad de muestra es aproximadamente el doble de la varianza de la muestra completa. Dado que se tienen dos mitades, se puede usar la diferencia entre sus estimaciones para calcular la varianza:

$$
\text{Var}(\hat{\theta}) \approx \frac{1}{2} (\hat{\theta}_A - \hat{\theta}_B)^2,
$$

donde $\hat{\theta}_A$ y $\hat{\theta}_B$ son las estimaciones de cada mitad de la muestra. Este enfoque es sencillo pero puede ser inestable, por lo que se suelen usar múltiples conjuntos de divisiones para obtener un promedio más preciso.

#### Balanced Repeated Replication (BRR) {.unnumbered}

El método de **Balanced Repeated Replication (BRR)** es una forma sistemática de elegir subconjuntos de la muestra, garantizando que cada unidad se incluya de manera equilibrada en las réplicas. Esto se logra mediante un balanceo ortogonal, donde cada observación está presente en aproximadamente la mitad de las réplicas, y cada par de unidades de diferentes estratos aparece en las réplicas de forma equilibrada. Con (K) estratos, se puede generar un conjunto de hasta (K + 4) réplicas que produzca una estimación de la varianza que es prácticamente idéntica a la que se obtendría usando todas las (2^K) combinaciones posibles.

La varianza utilizando BRR se calcula así:

$$
\text{Var}_{\text{BRR}}(\hat{\theta}) = \frac{1}{R} \sum_{r=1}^R (\hat{\theta}_r - \hat{\theta})^2,
$$

donde $R$ es el número de réplicas seleccionadas y $\hat{\theta}_r$ es el estimador obtenido de cada réplica.

#### Pesos Replicados en Diseños Multietápicos y Complejos {.unnumbered}

El enfoque de pesos replicados no solo se aplica a diseños simples, sino que también se adapta a **diseños de muestreo multietápicos** y **diseños complejos**. En estos casos, la estructura de la muestra se complica, ya que puede involucrar varias etapas de selección (por ejemplo, seleccionar primero conglomerados como municipios, luego hogares dentro de los municipios, y finalmente personas dentro de los hogares). Esto hace que la varianza deba considerar la correlación entre unidades seleccionadas en cada etapa.

Para estos diseños, se utilizan métodos como el **Jackknife** y el **Bootstrap**, que permiten manejar la estructura multietápica. Por ejemplo:

-   En un diseño **Jackknife**, se ajustan los pesos eliminando una observación o un conglomerado completo en cada réplica, y recalculando el estimador con los datos restantes. Esto puede ajustarse para considerar la estructura de estratos y conglomerados.

$$
\text{Var}_{\text{Jackknife}}(\hat{\theta}) = \frac{n-1}{n} \sum_{i=1}^n (\hat{\theta}_i - \hat{\theta})^2
$$

donde (n) es el número de observaciones o conglomerados, $\hat{\theta}_i$ es la estimación obtenida cuando se omite la $i$-ésima unidad, y $\hat{\theta}$ es la estimación con todos los datos.

-   En el **Bootstrap**, se seleccionan subconjuntos con reemplazo de cada conglomerado, y se ajustan los pesos según el número de veces que cada unidad aparece en la réplica. Esto es especialmente útil cuando las unidades de muestreo tienen una estructura jerárquica, como es el caso de los diseños multietápicos.

$$
\text{Var}_{\text{Bootstrap}}(\hat{\theta}) = \frac{1}{B} \sum_{b=1}^B (\hat{\theta}_b - \hat{\theta})^2,
$$

donde $B$ es el número de réplicas y $\hat{\theta}_b$ es el estimador obtenido en la $b$-ésima réplica.

#### Ventajas de los Pesos Replicados {.unnumbered}

Aunque estos métodos requieren más esfuerzo computacional comparados con métodos tradicionales como el estimador de Horvitz-Thompson, son muy versátiles. Facilitan la estimación de errores estándar para diferentes tipos de estadísticas, no solo para medias o totales, y son especialmente útiles cuando se trabaja con diseños de muestreo complejos. Además, permiten obtener errores estándar precisos para estimaciones de subpoblaciones sin necesidad de ajustes adicionales. Esto los convierte en una herramienta poderosa para el análisis de encuestas complejas, especialmente con el soporte de software estadístico moderno.

El paquete `survey` con `svrep` proporciona una implementación robusta de varios métodos de pesos replicados, incluyendo Balanced Repeated Replication (BRR), Jackknife, y Bootstrap. Sin embargo, el uso adecuado de estos métodos a menudo no es tan conocido por usuarios que no son expertos en muestreo. La correcta especificación del diseño y la interpretación de los resultados pueden ser complejas, especialmente en el caso de diseños de muestreo multietápicos o aquellos que requieren calibración.

Dentro de `metasurvey` se busca simplificar el uso de estos métodos, pudiendo especificar el tipo de réplica deseado con un solo argumento al cargar la encuesta o utilizar réplicas brindadas por la institución que publica los microdatos. Además, se busca incorporar medidas de calidad de las estimaciones como el coeficiente de variación, el error relativo y el error absoluto, para facilitar la interpretación de los resultados y la comparación entre diferentes estimaciones y subpoblaciones.

#### Medidas de calidad de las estimaciones {.unnumbered}

Brindar medidas de calidad de las estimaciones es fundamental para evaluar la confiabilidad de los resultados y comparar diferentes estimaciones. Estas medidas permiten cuantificar la precisión de los estimadores y evaluar la variabilidad de los resultados. Algunas de las medidas más comunes son el **coeficiente de variación** (CV) y el **efecto diseño** (*deff*), ya que en algunos casos, si bien se pueden obtener estimaciones, estas pueden no ser precisas o el tamaño de la muestra en el dominio de interés puede ser pequeño, lo que puede llevar a estimaciones poco confiables.

#### Herramientas para la estimación de varianza {.unnumbered}

Si bien para una persona con experiencia en muestreo estas medidas son familiares, para un usuario no experto, en caso de no contar con herramientas que permitan calcular estas medidas, se pueden pasar por alto. En este sentido, es importante contar con herramientas que permitan calcular estas medidas de forma sencilla y eficiente, para facilitar la interpretación de los resultados y la toma de decisiones.

En la actualidad, existen diferentes métodos para la estimación de varianzas. Aunque en la mayoría de los casos se utilizan métodos de remuestreo como el Bootstrap o el Jackknife, existen diferentes ideas o propuestas como se menciona en [@deville1998] y [@deville2005], que demuestran con resultados numéricos que estimadores del tipo **H-T** bajo un diseño balanceado pueden aproximarse desde el enfoque de regresión o calibración. Además, existen estimadores alternativos que complementan métodos de remuestreo para aproximar probabilidades de inclusión de segundo orden [@escobar2013] utilizando ciertas aproximaciones límites [@hajek1964].

En este sentido, `metasurvey` busca ser una herramienta que permita a usuarios no expertos en muestreo realizar análisis de encuestas de manera sencilla, proporcionando una interfaz amigable y herramientas que faciliten la interpretación de los resultados. Además, busca integrar diferentes métodos de estimación de varianzas y medidas de calidad de las estimaciones, para que los usuarios puedan obtener resultados confiables y precisos, sin necesidad de ser expertos en muestreo.


En el [Capítulo @sec-metodos-computacionales] se presentarán conceptos relacionados al desarrollo de paquetes en R junto a la presentación de diferentes herramientas para desarrollar paquetes. Se mencionarán paquetes que permiten realizar estimaciones de varianzas y que complementarán el paquete `metasurvey` así como también paquetes similares o trabajos similares en el [Capítulo @sec-Antecedentes]. En el [Capítulo @sec-Desarrollo-metodologia] se presentará cómo se han integrado los conceptos de muestreo y estimación de varianzas, en conjunto con las dependencias e implementaciones realizadas en el paquete `metasurvey` para facilitar el proceso de obtención de indicadores socioeconómicos y demográficos.


